{
  "name": "Practical Machine Learning Course",
  "tagline": "Project Assigment",
  "body": "# How well are we doing our Physical exercise?\r\n***************************************************\r\n\r\n###Summary:\r\nNowadays, to improve the life quality more and more peole are practicing sports. The practicing not only improve the body physical conditions or fitness but also decrease the likehood of many ills related to sedentarism, such as heart diseases. But, it is not only the act or quantity of doing physical exercise that matters. Many people usually forget that physical exercises when are done in the wrong way, they are, on one hand, less effective. On the other hand they can be very damaging causing undesirable injuries. Therefore, the right way to perform the exercise is considered a top priority.\r\nThis project is based on the work made by Veloso *et al.* \"Qualitative Activity Recognition of Weight Lifting Exercises\". The authors have mounted sensors in six male volunteers to lift a relatively light dumbbell (1.25kg). Five different ways (only one correct) to perform the lift exercise were monitored by the sensors. The data collected were analysed and a machine learning model was built to assess the correctness and feedbacking the user at real-time; increasing the likewood of the exercise effectiviness. \r\n\r\n###Scope:\r\nAs part of the Coursera assessment, the report described here are restricted to answer the followings:\r\n\r\n*   Predict the manner in which the exercise was done.\r\n*   Show how cross validation was implemented\r\n*   Analyse the sample error.\r\n*   Discuss the assumptions made.\r\n*   Apply the proposed model to predict 20 different test cases.\r\n\r\n###Experiment description: \r\nSix (6) volunteers weared four (4) \"9 Degrees of Freedom - Razor IMU\". Each one of Razor IMU is composed of three sensors: accelerometer, gyroscope and magnetometer, in which of them provides 3 degrees of freedom. Therefore, a total of 9 degrees of freedom per location. The four locations were:\r\n\r\n*   glove\r\n*   armband\r\n*   lumbar belt\r\n*   dumbbell \r\n\r\nThe volunteers were male participants aged between 20-28 years. They performed one set of 10 repetitions of the activity \"Unilateral Dumbbell Biceps Curl\" in five different \"classes\", one class is the right way and the others in wrong way:\r\n- Class A - The rigth exercise (*i.e*., exactly according to the specification)\r\n- Class B - Throwing the elbows to the front\r\n- Class C - Lifting the dumbbell only halfway \r\n- Class D - Lowering the dumbbell only halfway\r\n- Class E - Throwing the hips to the front\r\n\r\n###Exploratory Data analysis:\r\n\r\n**1.\tDownload the data from a provided URL.**\r\n\r\n```R\r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-training.csv\")\r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-testing.csv\")\r\n```\r\n\r\n**2.\tLoad to R Global environment the files from their respective folders.**\r\n\r\n```R\r\n    pml.training <- read.csv(\"pml-training.csv\",header=TRUE, na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n    pml.testing  <- read.csv(\"pml-testing.csv\", header=TRUE, na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n```\r\n**3.\tTake a look at the data and analyse the data structure**\r\n\r\n```R\r\nstr(pml.training, list.len=20)\r\n\r\n'data.frame':\t19622 obs. of  160 variables:\r\n $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ user_name               : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...\r\n $ raw_timestamp_part_2    : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...\r\n $ cvtd_timestamp          : Factor w/ 20 levels \"02/12/2011 13:32\",..: 9 9 9 9 9 9 9 9 9 9 ...\r\n $ new_window              : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...\r\n $ roll_belt               : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...\r\n $ pitch_belt              : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...\r\n $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...\r\n $ total_accel_belt        : int  3 3 3 3 3 3 3 3 3 3 ...\r\n $ kurtosis_roll_belt      : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ kurtosis_picth_belt     : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ kurtosis_yaw_belt       : logi  NA NA NA NA NA NA ...\r\n $ skewness_roll_belt      : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ skewness_roll_belt.1    : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ skewness_yaw_belt       : logi  NA NA NA NA NA NA ...\r\n $ max_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ max_picth_belt          : int  NA NA NA NA NA NA NA NA NA NA ...\r\n $ max_yaw_belt            : num  NA NA NA NA NA NA NA NA NA NA ...\r\n  [list output truncated]\r\n```\r\n\r\nRaw data variables for each sensor per location recorded as follow:\r\n*   Triaxial accelerometer:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n*   Triaxial gyroscope:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n*   Triaxial  magnetometer:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n\r\nA total of 36 raw data variables were then recorded.\r\n\r\nIt was recorded also \"fusion\" variables obtained form the raw data variables:\r\n*   Three Rotational angles (roll, pitch, yaw) per location:\r\n    -   3  x 4 (locations) = 12\r\n*   Total acceleration per location:\r\n    -   1  x 4 (locations) = 4\r\n\r\nThe data of accelerometer, magnetometer and gyroscope are \"fused\" by using a Direction Cosine Matrix (DCM) algorithm in the Razor IMU, that means a total of 16 \"fused\" variables are recorded. Furthermore, in some of \"fused\" variables, the following *descriptive statistics* variables (\"stat\") were recorded:\r\n*   Max, Min, standard deviation(stddev), variance(var), average(avg) of rotational angles:\r\n    -   5 x 12 = 60      \r\n*   Amplitude of rotational angles (amplitude):\r\n    -   3 x 4  = 12\r\n*   kurtosis and skewness of rotational angles:\r\n    -   2 x 12 = 24\r\n*   variance(var) of total acceleration:\r\n    -   1 x 4  = 4\r\n\r\nA total of 100 \"stat\" variables.\r\n\r\nThe remainder variables:\r\n*   Volunteer ID (user_name)\r\n*   Exercise type (class)\r\n*   Index or observable number (X)\r\n*   time & date (cvtd_timestamp)\r\n*   Four (4) time variables (raw_timestample_part_1 & raw_timestample_part_1, new_window, num_window);\r\n\r\nA total of 8 remainder variables.\r\nIn summary, the experiment recorded a total number of 160 (36 + 16 + 100 + 8) variables.\r\n\r\nEach individual performed 5 \"classes\" of exercises. Each exercise was repeat 10 times. All together derived a total of 19642 recorded observables, which were then split by Coursera into two files:\r\n*  train set to create the Model loaded as \"pml.training\" is a dataframe of 19622 x 160 \r\n*  test set to answer the Quiz loaded as \"pml.testing\"  is a dataframe of 20 x 160\r\n\r\n**4.  Data Cleaning**\r\n\r\nThe data was cleaned and the variables re-labelled by using *CamelCase* structure.\r\n```R\r\n    temp=names(pml.training)\r\n    temp=gsub(\"^a\",\"A\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^g\",\"G\",temp); \r\n    temp=gsub(\"^k\",\"K\",temp); temp=gsub(\"^m\",\"M\",temp); temp=gsub(\"^n\",\"N\",temp); temp=gsub(\"^p\",\"P\",temp);\r\n    temp=gsub(\"^r\",\"R\",temp); temp=gsub(\"^s\",\"S\",temp); temp=gsub(\"^t\",\"T\",temp); temp=gsub(\"^u\",\"U\",temp);\r\n    temp=gsub(\"^v\",\"V\",temp); temp=gsub(\"^y\",\"Y\",temp); temp=gsub(\"_a\",\"A\",temp); temp=gsub(\"_b\",\"B\",temp);\r\n    temp=gsub(\"_c\",\"C\",temp); temp=gsub(\"_d\",\"D\",temp); temp=gsub(\"_f\",\"F\",temp); temp=gsub(\"_g\",\"G\",temp);\r\n    temp=gsub(\"_k\",\"K\",temp); temp=gsub(\"_m\",\"M\",temp); temp=gsub(\"_n\",\"N\",temp); temp=gsub(\"_p\",\"P\",temp);\r\n    temp=gsub(\"_r\",\"R\",temp); temp=gsub(\"_s\",\"S\",temp); temp=gsub(\"_t\",\"T\",temp); temp=gsub(\"_u\",\"U\",temp);\r\n    temp=gsub(\"_v\",\"V\",temp); temp=gsub(\"_x\",\"X\",temp); temp=gsub(\"_y\",\"Y\",temp); temp=gsub(\"_w\",\"W\",temp);\r\n    temp=gsub(\"_z\",\"Z\",temp); temp=gsub(\"_1\",\"1\",temp); temp=gsub(\"_2\",\"2\",temp);\r\n    TidyData = pml.training\r\n    colnames(TidyData) = temp\r\n```\r\nAlso, form the data structure we observed that there are missing values (\"NA\") in the data. We can check if the proportion is relevant by \r\n```R\r\n    mean(is.na(TidyData))\r\n    [1] 0.6131835\r\n```\r\nOver 61% of the data is missed, which is a very significant proportion. So, we can set a threshold to remove variables that contain more than 95% of \"NA\", for example:\r\n```R\r\n    NewTidyData1 = TidyData[, colSums(is.na(TidyData))/nrow(TidyData) < 0.95]\r\n    dim(NewTidyData1)\r\n    [1] 19622    60\r\n    \r\n    mean(is.na(NewTidyData1))\r\n    [1] 0\r\n```\r\nThe `NewTidyData1` now does not contain any missed value. The first 20 rows of data structure of the `NewTidyData1`:\r\n```R\r\n str(NewTidyData1, list.len=20)\r\n'data.frame':\t19622 obs. of  60 variables:\r\n $ X                 : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ UserName          : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n $ RawTimestampPart1 : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...\r\n $ RawTimestampPart2 : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...\r\n $ CvtdTimestamp     : Factor w/ 20 levels \"02/12/2011 13:32\",..: 9 9 9 9 9 9 9 9 9 9 ...\r\n $ NewWindow         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ NumWindow         : int  11 11 11 12 12 12 12 12 12 12 ...\r\n $ RollBelt          : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...\r\n $ PitchBelt         : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...\r\n $ YawBelt           : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...\r\n $ TotalAccelBelt    : int  3 3 3 3 3 3 3 3 3 3 ...\r\n $ GyrosBeltX        : num  0 0.02 0 0.02 0.02 0.02 0.02 0.02 0.02 0.03 ...\r\n $ GyrosBeltY        : num  0 0 0 0 0.02 0 0 0 0 0 ...\r\n $ GyrosBeltZ        : num  -0.02 -0.02 -0.02 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02 0 ...\r\n $ AccelBeltX        : int  -21 -22 -20 -22 -21 -21 -22 -22 -20 -21 ...\r\n $ AccelBeltY        : int  4 4 5 3 2 4 3 4 2 4 ...\r\n $ AccelBeltZ        : int  22 22 23 21 24 21 21 21 24 22 ...\r\n $ MagnetBeltX       : int  -3 -7 -2 -6 -6 0 -4 -2 1 -3 ...\r\n $ MagnetBeltY       : int  599 608 600 604 600 603 599 603 602 609 ...\r\n $ MagnetBeltZ       : int  -313 -311 -305 -310 -302 -312 -311 -313 -312 -308 ...\r\n  [list output truncated]    \r\n```\r\nFrom the initial 160, there are a total of 60 variables. We can still reduce the number of variables **_assuming_** that the prediction is not time and individual dependent, that is, independent of the person and the time that the exercised was performed. Then, variables such as: *\"UserName\", \"RawTimestampPart1\", \"RawTimestampPart2\", \"CvtdTimestamp\", \"NewWindow\" and \"NumWindow\"* can be removed together with the index *\"X\"* variable:\r\n```R\r\n    library(dplyr)\r\n    SelectData = select(NewTidyData1,-c(X:NumWindow))\r\n    dim(SelectData)\r\n    [1] 19622    53\r\n```\r\nTherefore, the number of variables is reduced to 53.\r\n\r\n**5.\tPropose Machine learning models base on the exploratory data**\r\n\r\nI used the *caret* package in this work. Caret stands for Classification And REgression Training. It is a great toolkit to build  classifycation and regression models. Caret also provides means for: Data preparation, Data splitting, Training a Model, Model evaluation, and  Variable selection.\r\n\r\n5.1.    Data Preparation - Removing redudant variables by a correlation matrix\r\n\r\nThe data variables may be correlated to each other, which it may lead to rendundancy in the model (*_assumption_*). By using `findCorrelation` from the *caret* package, we can obtain the correlation matrix of between the data variables. The function \r\ncan plot the the entiry data set to visualise those correlations (See Figure 1).  The plot makes less difficult the choice for threshold of the correlation coefficient in  order to remove the redundant variables. I choose that a absolute value for correlation coefficient of 0.90 as the threshold. Seven (7) other variables can be dropped.\r\n```R\r\n    library(caret)\r\n    threshold   <-  0.90\r\n    corMatrix   <-  cor(SelectData[,1:52])\r\n    corrplot(corMatrix, method=\"square\", order=\"hclust\") # Plot the correlation matrix\r\n    \r\n    highCor <-  findCorrelation(corMatrix, threshold) #sub set those that thas correlation >, = 0.9, or < -0.9\r\n    highCorRm   <-  row.names(corMatrix)[highCor]\r\n    highCorRm\r\n    [1] \"AccelBeltZ\"    \"RollBelt\"       \"AccelBeltY\"     \"AccelBeltX\"     \r\n        \"GyrosDumbbellX\" \"GyrosDumbbellZ\"  \"GyrosArmX\"\r\n\r\n    SelectData2 <- SelectData[, -highCor]\r\n    dim(SelectData2)\r\n    [1] 19622    46\r\n```\r\n<p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/anonymous-1618/ML/master/Fig1.png\">\r\n  <b>Figure 1 - </b>Variables Correlation Map</b><br>\r\n  </p>\r\n\r\n**ii) Data spliting**\r\n\r\nThe *caret* function `createDataPartition` is used to randomly split the data set. I set the standard proportion of 60% of the data to be used for model training and 40% used for testing model performance.\r\n```R\r\n    library(\"caret\")\r\n    library(\"e1071\")\r\n    set.seed(123)\r\n    inTrain <- createDataPartition(SelectData$Class, p = 0.6, list = FALSE)\r\n    trainData <- SelectData[inTrain,]\r\n    testData <- SelectData[-inTrain,]\r\n    modelFit1 = train(Classe ~., data=trainData, method=\"rf\", prox=TRUE)\r\n\r\n    library(\"caret\")\r\n    set.seed(1023)\r\n    inTrain <- createDataPartition(SelectData2$Class, p = 0.6, list = FALSE)\r\n    trainData <- SelectData2[inTrain,]\r\n    testData <- SelectData2[-inTrain,]\r\n    modelFit2 = train(Classe ~., data=trainData, method=\"rf\", prox=TRUE)\r\n```\r\n**iii) Training a Model/Tuning Parameters/Building the Final model**\r\n\r\nAs the main question of this assigment is about classification, I choose Random Forest (\"rf\") to build the model. Tuning the model means to choose a set of parameters to be evaluated. Once the model and tuning parameters are choosen, the type of resampling (cross-validation) need to be opted. \r\nCaret Package has tools to perfom, k-fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap resampling. I worked with two type of resampling to evaluate the performance: Bootstrap (default) and k-fold cross-validation (once or repeated). Once the resampling was processed, the caret `train` function automatically chooses the best tuning parameters associated to the model.\r\n\r\nUsing correlation to reduce number of variable and eliminating UserName\r\n\r\n```R\r\n    modelFit2\r\n    11776 samples\r\n    45 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\n    No pre-processing\r\n    Resampling: Bootstrapped (25 reps) \r\n    Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... \r\n    Resampling results across tuning parameters:\r\n\r\n    mtry  Accuracy   Kappa    \r\n    2    0.9856378  0.9818241\r\n    23    0.9874277  0.9840903\r\n    45    0.9752460  0.9686771\r\n\r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 23. \r\n\r\n> modelFit2$finalModel\r\n\r\nCall:\r\n randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) \r\n               Type of random forest: classification\r\n                     Number of trees: 500\r\nNo. of variables tried at each split: 23\r\n\r\n        OOB estimate of  error rate: 0.91%\r\n    Confusion matrix:\r\n         A    B    C    D    E    class.error\r\n    A 3338    6    1    1    2  0.002986858\r\n    B   22 2243   14    0    0  0.015796402\r\n    C    0   14 2031    9    0  0.011197663\r\n    D    1    0   25 1903    1  0.013989637\r\n    E    0    1    2    8 2154  0.005080831\r\n\r\n    varImp(modelFit2, scale=TRUE)\r\n    rf variable importance\r\n\r\n    only 20 most important variables shown (out of 45)\r\n\r\n                       Overall\r\n    YawBelt             100.00\r\n    PitchForearm         86.89\r\n    PitchBelt            71.89\r\n    MagnetDumbbellZ      63.81\r\n    MagnetDumbbellY      51.95\r\n    RollForearm          50.97\r\n    MagnetBeltY          46.42\r\n    MagnetBeltZ          29.86\r\n    GyrosBeltZ           28.77\r\n    MagnetDumbbellX      28.23\r\n    AccelDumbbellY       27.33\r\n    RollDumbbell         27.13\r\n    AccelForearmX        21.55\r\n    MagnetBeltX          19.06\r\n    AccelDumbbellZ       19.02\r\n    TotalAccelDumbbell   18.92\r\n    AccelForearmZ        17.06\r\n    MagnetForearmZ       15.57\r\n    TotalAccelBelt       15.14\r\n    YawArm               14.18\r\n```\r\n\r\n**iv) Model Evaluation**\r\n```R\r\n    testPred <- predict(modelFit2, newdata = testData)\r\n    confusionMatrix(testData$Classe,testPred)\r\n\r\n    Confusion Matrix and Statistics\r\n\r\n              Reference\r\n    Prediction     A    B    C    D    E\r\n         A      2232    0    0    0    0\r\n         B         5 1505    8    0    0\r\n         C         0    5 1361    2    0\r\n         D         0    1   22 1261    2\r\n         E         0    1    3    4 1434\r\n\r\n    Overall Statistics\r\n                                          \r\n               Accuracy : 0.9932          \r\n                 95% CI : (0.9912, 0.9949)\r\n    No Information Rate : 0.2851          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9915          \r\n    Mcnemar's Test P-Value : NA              \r\n\r\n    Statistics by Class:\r\n\r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9978   0.9954   0.9763   0.9953   0.9986\r\n    Specificity            1.0000   0.9979   0.9989   0.9962   0.9988\r\n    Pos Pred Value         1.0000   0.9914   0.9949   0.9806   0.9945\r\n    Neg Pred Value         0.9991   0.9989   0.9949   0.9991   0.9997\r\n    Prevalence             0.2851   0.1927   0.1777   0.1615   0.1830\r\n    Detection Rate         0.2845   0.1918   0.1735   0.1607   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9989   0.9967   0.9876   0.9957   0.9987\r\n```\r\n\r\n####### after model fit\r\n\r\n    temp =varImp(modelFit2, scale=TRUE)\r\n    temp.df =as.data.frame(temp[[1]])\r\n    temp.df = cbind(Variables = rownames(temp.df),temp.df)\r\n    ####temp.df = arrange(temp.df,desc(Overall))\r\n    ####filtering to variables with importance > 10%\r\n    Impvar.names = as.vector(filter(temp.df, Overall > 10)[,1])\r\n    selectVar = c(Impvar.names,\"Classe\")\r\n        \r\n    > Impvar.names\r\n         [1] \"PitchBelt\"          \"YawBelt\"            \"TotalAccelBelt\"     \"GyrosBeltZ\"        \r\n         [5] \"MagnetBeltX\"        \"MagnetBeltY\"        \"MagnetBeltZ\"        \"RollArm\"           \r\n         [9] \"YawArm\"             \"RollDumbbell\"       \"YawDumbbell\"        \"TotalAccelDumbbell\"\r\n        [13] \"GyrosDumbbellY\"     \"AccelDumbbellY\"     \"AccelDumbbellZ\"     \"MagnetDumbbellX\"   \r\n        [17] \"MagnetDumbbellY\"    \"MagnetDumbbellZ\"    \"RollForearm\"        \"PitchForearm\"      \r\n        [21] \"AccelForearmX\"      \"AccelForearmZ\"      \"MagnetForearmZ\"  \r\n        \r\n    trainDataImp = trainData[,Impvar.names]\r\n    modelFit2a = train(Classe ~., data=trainDataImp, method=\"rf\", prox=TRUE)\r\n    date()\r\n        \r\n    [1] \"Tue Jun 28 23:19:44 2016\"\r\n    > modelFit2a = train(Classe ~., data=trainDataImp, method=\"rf\", prox=TRUE)\r\n    There were 50 or more warnings (use warnings() to see the first 50)\r\n    >   date()\r\n    [1] \"Wed Jun 29 05:33:39 2016\"\r\n    >   save.image(\"~/machinelearning4.RData\")\r\n    >   modelFit2a\r\n        Random Forest \r\n        \r\n        11776 samples\r\n           23 predictor\r\n            5 classes: 'A', 'B', 'C', 'D', 'E' \r\n        \r\n        No pre-processing\r\n        Resampling: Bootstrapped (25 reps) \r\n        Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... \r\n        Resampling results across tuning parameters:\r\n        \r\n          mtry  Accuracy   Kappa    \r\n           2    0.9815209  0.9765947\r\n          12    0.9816863  0.9768206\r\n          23    0.9697268  0.9616665\r\n        \r\n        Accuracy was used to select the optimal model using  the largest value.\r\n        The final value used for the model was mtry = 12. \r\n        \r\n    >   modelFit2a$finalModel\r\n        \r\n        Call:\r\n        randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) \r\n                       Type of random forest: classification\r\n                             Number of trees: 500\r\n        No. of variables tried at each split: 12\r\n        \r\n                OOB estimate of  error rate: 1.07%\r\n        Confusion matrix:\r\n             A    B    C    D    E class.error\r\n        A 3337    7    1    1    2 0.003285544\r\n        B   22 2230   24    1    2 0.021500658\r\n        C    0   15 2021   18    0 0.016066212\r\n        D    1    1   19 1907    2 0.011917098\r\n        E    0    1    3    6 2155 0.004618938\r\n        \r\n        > testPred <- predict(modelFit2a, newdata = testData)\r\n        > confusionMatrix(testData$Classe,testPred)\r\n        Confusion Matrix and Statistics\r\n        \r\n                  Reference\r\n        Prediction    A    B    C    D    E\r\n                 A 2231    1    0    0    0\r\n                 B    5 1499   13    1    0\r\n                 C    0    4 1355    9    0\r\n                 D    0    1   21 1260    4\r\n                 E    0    0    3    5 1434\r\n        \r\n        Overall Statistics\r\n                                                  \r\n                       Accuracy : 0.9915          \r\n                         95% CI : (0.9892, 0.9934)\r\n            No Information Rate : 0.285           \r\n            P-Value [Acc > NIR] : < 2.2e-16       \r\n                                                  \r\n                          Kappa : 0.9892          \r\n         Mcnemar's Test P-Value : NA              \r\n        \r\n        Statistics by Class:\r\n        \r\n                             Class: A Class: B Class: C Class: D Class: E\r\n        Sensitivity            0.9978   0.9960   0.9734   0.9882   0.9972\r\n        Specificity            0.9998   0.9970   0.9980   0.9960   0.9988\r\n        Pos Pred Value         0.9996   0.9875   0.9905   0.9798   0.9945\r\n        Neg Pred Value         0.9991   0.9991   0.9943   0.9977   0.9994\r\n        Prevalence             0.2850   0.1918   0.1774   0.1625   0.1833\r\n        Detection Rate         0.2843   0.1911   0.1727   0.1606   0.1828\r\n        Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n        Balanced Accuracy      0.9988   0.9965   0.9857   0.9921   0.9980\r\n        \r\n-------------------------------------------------------------\r\n\r\n\r\nFor the train function, the possible resampling methods (cross-validatios) are:\r\n\r\n* bootstrapping,\r\n* k-fold cross-validation,\r\n* leave-one-out cross-validation,\r\n* leave-group-out cross-validation (i.e., repeated splits without replacement).\r\n\r\n#######using k-fold CV \r\nIn this work two cross-validations will be evaluated: Boststrapping and k-fold cross-validation\r\n\r\n#######Predictor importance\r\nThe generic function varImp can be used to characterize the general selection of predictors on\r\nthe model. The varImp function works with the following object classes: lm, mars, earth,\r\nrandomForest, gbm, mvr (in the pls package), rpart, RandomForest (from the party package),\r\npamrtrained, bagEarth, bagFDA, classbagg and regbagg. varImp also works with objects\r\nproduced by train, but this is a simple wrapper for the speci\fc models previously listed\r\n\r\n    date()\r\n    set.seed(2825)\r\n    fitControl <- trainControl( method = \"cv\", number = 10)\r\n    modelFit2b <- train(Classe ~ ., data = trainDataImp, method=\"rf\", ntree=200, trControl = fitControl)\r\n    date()\r\n    > date()\r\n    [1] \"Wed Jun 29 19:38:18 2016\"\r\n    > set.seed(2825)\r\n    > fitControl <- trainControl( method = \"cv\", number = 10)\r\n    > modelFit2b <- train(Classe ~ ., data = trainDataImp, method=\"rf\", ntree=200, trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 19:44:06 2016\"\r\n    > \r\n    > modelFit2b\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10598, 10599, 10599, 10597, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9866676  0.9831335\r\n      12    0.9871779  0.9837805\r\n      23    0.9774111  0.9714243\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n    \r\n    \r\n    \r\n    > plot(modelFit2b$finalModel)\r\n    > plot(modelFit2b)\r\n    > testPred <- predict(modelFit2b, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2230    2    0    0    0\r\n             B    6 1495   16    1    0\r\n             C    0    4 1354   10    0\r\n             D    0    0   20 1263    3\r\n             E    0    0    5    5 1432\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9908          \r\n                     95% CI : (0.9885, 0.9928)\r\n        No Information Rate : 0.285           \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9884          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9960   0.9706   0.9875   0.9979\r\n    Specificity            0.9996   0.9964   0.9978   0.9965   0.9984\r\n    Pos Pred Value         0.9991   0.9848   0.9898   0.9821   0.9931\r\n    Neg Pred Value         0.9989   0.9991   0.9937   0.9976   0.9995\r\n    Prevalence             0.2850   0.1913   0.1778   0.1630   0.1829\r\n    Detection Rate         0.2842   0.1905   0.1726   0.1610   0.1825\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9985   0.9962   0.9842   0.9920   0.9982\r\n    \r\n    \r\n    > date()\r\n    [1] \"Wed Jun 29 19:38:18 2016\"\r\n    > set.seed(2825)\r\n    > fitControl <- trainControl( method = \"cv\", number = 10)\r\n    > modelFit2b <- train(Classe ~ ., data = trainDataImp, method=\"rf\", ntree=200, trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 19:44:06 2016\"\r\n    > \r\n    > modelFit2b\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10598, 10599, 10599, 10597, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9866676  0.9831335\r\n      12    0.9871779  0.9837805\r\n      23    0.9774111  0.9714243\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n    > plot(modelFit2b$finalModel)\r\n    > plot(modelFit2b)\r\n    > testPred <- predict(modelFit2b, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2230    2    0    0    0\r\n             B    6 1495   16    1    0\r\n             C    0    4 1354   10    0\r\n             D    0    0   20 1263    3\r\n             E    0    0    5    5 1432\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9908          \r\n                     95% CI : (0.9885, 0.9928)\r\n        No Information Rate : 0.285           \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9884          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9960   0.9706   0.9875   0.9979\r\n    Specificity            0.9996   0.9964   0.9978   0.9965   0.9984\r\n    Pos Pred Value         0.9991   0.9848   0.9898   0.9821   0.9931\r\n    Neg Pred Value         0.9989   0.9991   0.9937   0.9976   0.9995\r\n    Prevalence             0.2850   0.1913   0.1778   0.1630   0.1829\r\n    Detection Rate         0.2842   0.1905   0.1726   0.1610   0.1825\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9985   0.9962   0.9842   0.9920   0.9982\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, ntree = 200, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 200\r\n    No. of variables tried at each split: 12\r\n    \r\n            OOB estimate of  error rate: 1.26%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3328   14    2    2    2 0.005973716\r\n    B   24 2230   23    1    1 0.021500658\r\n    C    0   20 2016   18    0 0.018500487\r\n    D    1    0   20 1902    7 0.014507772\r\n    E    0    0    3   10 2152 0.006004619\r\n    \r\n    > date()\r\n    [1] \"Wed Jun 29 20:01:57 2016\"\r\n    > set.seed(1825)\r\n    > fitControl <- trainControl( method = \"cv\", number = 10)\r\n    > modelFit2c <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 20:19:25 2016\"\r\n    > \r\n    > modelFit2c\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10598, 10598, 10599, 10599, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9862432  0.9825951\r\n      12    0.9872615  0.9838864\r\n      23    0.9783446  0.9726068\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n    > plot(modelFit2c$finalModel)\r\n    > plot(modelFit2c)\r\n    > testPred <- predict(modelFit2c, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2231    1    0    0    0\r\n             B    7 1498   13    0    0\r\n             C    0    4 1354   10    0\r\n             D    0    2   18 1263    3\r\n             E    0    1    4    3 1434\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9916          \r\n                     95% CI : (0.9893, 0.9935)\r\n        No Information Rate : 0.2852          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9894          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9969   0.9947   0.9748   0.9898   0.9979\r\n    Specificity            0.9998   0.9968   0.9978   0.9965   0.9988\r\n    Pos Pred Value         0.9996   0.9868   0.9898   0.9821   0.9945\r\n    Neg Pred Value         0.9988   0.9987   0.9946   0.9980   0.9995\r\n    Prevalence             0.2852   0.1919   0.1770   0.1626   0.1832\r\n    Detection Rate         0.2843   0.1909   0.1726   0.1610   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9983   0.9958   0.9863   0.9932   0.9983\r\n    \r\n    > (modelFit2c$finalModel)\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 12\r\n    \r\n            OOB estimate of  error rate: 1.14%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3335    9    1    1    2 0.003882915\r\n    B   24 2231   24    0    0 0.021061869\r\n    C    0   21 2017   16    0 0.018013632\r\n    D    1    1   18 1908    2 0.011398964\r\n    E    0    1    4    9 2151 0.006466513\r\n    \r\n    \r\n    date()\r\n    set.seed(1325)\r\n    fitControl <- trainControl( method = \"cv\", number = 3)\r\n    modelFit2d <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    date()\r\n    > date()\r\n    [1] \"Wed Jun 29 21:14:59 2016\"\r\n    > set.seed(1325)\r\n    > fitControl <- trainControl( method = \"cv\", number = 3)\r\n    > modelFit2d <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 21:18:30 2016\"\r\n    > \r\n    > modelFit2d\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (3 fold) \r\n    Summary of sample sizes: 7852, 7851, 7849 \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9825917  0.9779763\r\n      12    0.9838655  0.9795928\r\n      23    0.9737600  0.9668130\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n    > modelFit2d$finalModel\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 12\r\n    \r\n            OOB estimate of  error rate: 1.1%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3336    8    1    1    2 0.003584229\r\n    B   23 2230   24    1    1 0.021500658\r\n    C    0   17 2024   13    0 0.014605648\r\n    D    0    2   20 1905    3 0.012953368\r\n    E    0    2    3    8 2152 0.006004619\r\n    > testPred <- predict(modelFit2d, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2231    1    0    0    0\r\n             B    6 1500   12    0    0\r\n             C    0    4 1354   10    0\r\n             D    0    2   20 1260    4\r\n             E    0    1    5    4 1432\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9912          \r\n                     95% CI : (0.9889, 0.9932)\r\n        No Information Rate : 0.2851          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9889          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9947   0.9734   0.9890   0.9972\r\n    Specificity            0.9998   0.9972   0.9978   0.9960   0.9984\r\n    Pos Pred Value         0.9996   0.9881   0.9898   0.9798   0.9931\r\n    Neg Pred Value         0.9989   0.9987   0.9943   0.9979   0.9994\r\n    Prevalence             0.2851   0.1922   0.1773   0.1624   0.1830\r\n    Detection Rate         0.2843   0.1912   0.1726   0.1606   0.1825\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9986   0.9959   0.9856   0.9925   0.9978\r\n    \r\n    \r\n    date()\r\n    set.seed(1025)\r\n    fitControl <- trainControl( method = \"cv\", number = 20)\r\n    modelFit2e <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    date()\r\n    > date()\r\n    [1] \"Wed Jun 29 21:23:51 2016\"\r\n    > set.seed(1025)\r\n    > fitControl <- trainControl( method = \"cv\", number = 20)\r\n    > modelFit2e <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 21:56:27 2016\"\r\n    > \r\n    > modelFit2e\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (20 fold) \r\n    Summary of sample sizes: 11188, 11186, 11187, 11187, 11188, 11187, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9873475  0.9839927\r\n      12    0.9881950  0.9850679\r\n      23    0.9790233  0.9734655\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n    > modelFit2e$finalModel\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 12\r\n    \r\n            OOB estimate of  error rate: 1.1%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3336    7    1    2    2 0.003584229\r\n    B   23 2231   25    0    0 0.021061869\r\n    C    0   19 2019   16    0 0.017039922\r\n    D    1    2   19 1906    2 0.012435233\r\n    E    0    0    3    8 2154 0.005080831\r\n    \r\n    > testPred <- predict(modelFit2e, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2232    0    0    0    0\r\n             B    7 1494   16    1    0\r\n             C    0    5 1353   10    0\r\n             D    0    2   20 1260    4\r\n             E    0    1    4    4 1433\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9906          \r\n                     95% CI : (0.9882, 0.9926)\r\n        No Information Rate : 0.2854          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9881          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9969   0.9947   0.9713   0.9882   0.9972\r\n    Specificity            1.0000   0.9962   0.9977   0.9960   0.9986\r\n    Pos Pred Value         1.0000   0.9842   0.9890   0.9798   0.9938\r\n    Neg Pred Value         0.9988   0.9987   0.9938   0.9977   0.9994\r\n    Prevalence             0.2854   0.1914   0.1775   0.1625   0.1832\r\n    Detection Rate         0.2845   0.1904   0.1724   0.1606   0.1826\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9984   0.9954   0.9845   0.9921   0.9979\r\n    \r\n    date()\r\n    set.seed(10025)\r\n    fitControl <- trainControl( method = \"cv\", number = 10)\r\n    modelFit2f <- train(Classe ~ ., data = trainData, method=\"rf\", trControl = fitControl)\r\n    date()\r\n    \r\n    > date()\r\n    [1] \"Wed Jun 29 22:17:02 2016\"\r\n    > set.seed(10025)\r\n    > fitControl <- trainControl( method = \"cv\", number = 10)\r\n    > modelFit2f <- train(Classe ~ ., data = trainData, method=\"rf\", trControl = fitControl)\r\n    > date()\r\n    [1] \"Wed Jun 29 22:48:25 2016\"\r\n    > \r\n    > modelFit2f\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       45 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10599, 10599, 10599, 10598, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9889613  0.9860343\r\n      23    0.9898949  0.9872163\r\n      45    0.9791109  0.9735707\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 23. \r\n    > modelFit2f$finalModel\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 23\r\n    \r\n            OOB estimate of  error rate: 0.86%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3340    5    1    0    2 0.002389486\r\n    B   23 2244   11    0    1 0.015357613\r\n    C    0   14 2031    9    0 0.011197663\r\n    D    1    1   23 1904    1 0.013471503\r\n    E    0    0    2    7 2156 0.004157044\r\n    > testPred <- predict(modelFit2f, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2232    0    0    0    0\r\n             B    6 1503    9    0    0\r\n             C    0    5 1362    1    0\r\n             D    0    0   24 1260    2\r\n             E    0    0    3    5 1434\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.993           \r\n                     95% CI : (0.9909, 0.9947)\r\n        No Information Rate : 0.2852          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9911          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9967   0.9742   0.9953   0.9986\r\n    Specificity            1.0000   0.9976   0.9991   0.9960   0.9988\r\n    Pos Pred Value         1.0000   0.9901   0.9956   0.9798   0.9945\r\n    Neg Pred Value         0.9989   0.9992   0.9944   0.9991   0.9997\r\n    Prevalence             0.2852   0.1922   0.1782   0.1614   0.1830\r\n    Detection Rate         0.2845   0.1916   0.1736   0.1606   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9987   0.9972   0.9867   0.9957   0.9987\r\n    > save.image(\"~/machinelearning5.RData\")\r\n    \r\n    > confusionMatrix(modelFit2f)\r\n    Cross-Validated (10 fold) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.3  0.2  0.0  0.0  0.0\r\n             B  0.1 19.0  0.1  0.0  0.0\r\n             C  0.0  0.1 17.2  0.2  0.0\r\n             D  0.0  0.0  0.1 16.2  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9899\r\n    \r\n    > confusionMatrix(modelFit2e)\r\n    Cross-Validated (20 fold) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.3  0.2  0.0  0.0  0.0\r\n             B  0.1 18.9  0.1  0.0  0.0\r\n             C  0.0  0.2 17.2  0.2  0.0\r\n             D  0.0  0.0  0.1 16.2  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9882\r\n    \r\n    > confusionMatrix(modelFit2d)\r\n    Cross-Validated (3 fold) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.2  0.2  0.0  0.0  0.0\r\n             B  0.1 18.9  0.2  0.0  0.0\r\n             C  0.1  0.2 17.1  0.3  0.0\r\n             D  0.0  0.0  0.2 16.0  0.1\r\n             E  0.0  0.0  0.0  0.1 18.2\r\n                                \r\n     Accuracy (average) : 0.9839\r\n    \r\n    > confusionMatrix(modelFit2c)\r\n    Cross-Validated (10 fold) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.3  0.2  0.0  0.0  0.0\r\n             B  0.1 18.9  0.2  0.0  0.0\r\n             C  0.0  0.2 17.1  0.2  0.0\r\n             D  0.0  0.0  0.2 16.2  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9873\r\n    \r\n    > confusionMatrix(modelFit2b)\r\n    Cross-Validated (10 fold) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.3  0.2  0.0  0.0  0.0\r\n             B  0.1 18.9  0.2  0.0  0.0\r\n             C  0.0  0.3 17.1  0.2  0.0\r\n             D  0.0  0.0  0.2 16.2  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9872\r\n    \r\n    > confusionMatrix(modelFit2a)\r\n    Bootstrapped (25 reps) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.3  0.3  0.0  0.0  0.0\r\n             B  0.2 18.6  0.2  0.0  0.0\r\n             C  0.0  0.3 16.9  0.3  0.1\r\n             D  0.0  0.0  0.2 16.0  0.1\r\n             E  0.0  0.0  0.0  0.0 18.4\r\n                                \r\n     Accuracy (average) : 0.9817\r\n    \r\n    > confusionMatrix(modelFit2)\r\n    Bootstrapped (25 reps) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.4  0.3  0.0  0.0  0.0\r\n             B  0.1 18.7  0.2  0.0  0.0\r\n             C  0.0  0.2 17.1  0.2  0.0\r\n             D  0.0  0.0  0.1 16.3  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9874\r\n     \r\n    \r\n    install.packages(\"doParallel\")\r\n    library(parallel)\r\n    library(doParallel)\r\n    cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS\r\n    registerDoParallel(cluster)\r\n    \r\n    date()\r\n    set.seed(12025)\r\n    fitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats=10, allowParallel = TRUE)\r\n    modelFit2g <- train(Classe ~ ., data = trainData, method=\"rf\", trControl = fitControl)\r\n    stopCluster(cluster)\r\n    date()\r\n    \r\n     cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS\r\n    > registerDoParallel(cluster)\r\n    > date()\r\n    [1] \"Wed Jun 29 23:11:05 2016\"\r\n    > set.seed(12025)\r\n    > fitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats=10, allowParallel = TRUE)\r\n    > modelFit2g <- train(Classe ~ ., data = trainData, method=\"rf\", trControl = fitControl)\r\n    Warning messages:\r\n    1: closing unused connection 8 (<-MobilePC:11421) \r\n    2: closing unused connection 7 (<-MobilePC:11421) \r\n    3: closing unused connection 6 (<-MobilePC:11421) \r\n    > stopCluster(cluster)\r\n    > date()\r\n    [1] \"Thu Jun 30 02:00:46 2016\"\r\n    > save.image(\"~/machinelearning5.RData\")\r\n    > modelFit2g\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       45 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold, repeated 10 times) \r\n    Summary of sample sizes: 10597, 10599, 10598, 10600, 10599, 10597, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9891898  0.9863233\r\n      23    0.9903194  0.9877536\r\n      45    0.9796197  0.9742176\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 23. \r\n    > modelFit2g$finalModel\r\n    \r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 23\r\n    \r\n            OOB estimate of  error rate: 0.87%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3338    7    1    0    2 0.002986858\r\n    B   24 2243   12    0    0 0.015796402\r\n    C    0   11 2033   10    0 0.010223953\r\n    D    1    0   23 1905    1 0.012953368\r\n    E    0    0    2    8 2155 0.004618938\r\n    > confusionMatrix(modelFit2g)\r\n    Cross-Validated (10 fold, repeated 10 times) Confusion Matrix \r\n    \r\n    (entries are percentual average cell counts across resamples)\r\n     \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 28.4  0.2  0.0  0.0  0.0\r\n             B  0.0 19.0  0.2  0.0  0.0\r\n             C  0.0  0.1 17.2  0.2  0.0\r\n             D  0.0  0.0  0.1 16.2  0.1\r\n             E  0.0  0.0  0.0  0.0 18.3\r\n                                \r\n     Accuracy (average) : 0.9903\r\n    \r\n    > testPred <- predict(modelFit2g, newdata = testData)\r\n    > confusionMatrix(testData$Classe,testPred)\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2232    0    0    0    0\r\n             B    6 1503    9    0    0\r\n             C    0    5 1360    3    0\r\n             D    0    0   21 1263    2\r\n             E    0    1    4    4 1433\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.993           \r\n                     95% CI : (0.9909, 0.9947)\r\n        No Information Rate : 0.2852          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9911          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9960   0.9756   0.9945   0.9986\r\n    Specificity            1.0000   0.9976   0.9988   0.9965   0.9986\r\n    Pos Pred Value         1.0000   0.9901   0.9942   0.9821   0.9938\r\n    Neg Pred Value         0.9989   0.9991   0.9948   0.9989   0.9997\r\n    Prevalence             0.2852   0.1923   0.1777   0.1619   0.1829\r\n    Detection Rate         0.2845   0.1916   0.1733   0.1610   0.1826\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9987   0.9968   0.9872   0.9955   0.9986 \r\n\r\n\r\n\r\n---------\r\n\r\n\r\nPre-Processing Options\r\n\r\n####Validation\r\n\r\n\r\n###Discussion\r\n\r\n\r\n###Conclusions:\r\nAs part of the Coursera assessment, the work described here are restricted to answer the followings:\r\n1.  To predict the manner in which the exercise was done.\r\n2.  To create a report describing how a proposed model was built (this present document).\r\n3.  To show how cross validation was implemented\r\n4.  To analyse the sample error.\r\n5.  To discuss the assumptions made.\r\n6.  To apply the proposed model to predict 20 different test cases.\r\n\r\n\r\n###Remarks:\r\n\r\n*  The variables were labeled within CamelCase structure. The variables description of the NewTidyData is presented in the Appendix B.\r\n*  Codes applied here are described in Appendix C.\r\n\r\n***************************************************\r\n####Reference:\r\nhttp://wiki.ros.org/razor_imu_9dof\r\nhttps://perceptual.mpi-inf.mpg.de/files/2013/03/velloso13_ah.pdf\r\nhttps://www.sparkfun.com/products/10736\r\nhttp://www.starlino.com/imu_guide.html\r\nhttp://www.aliexpress.com/item/DFRobot-9-degrees-of-freedom-inertial-navigation-sensors-9DOF-Razor-IMU-AHRS-compatib/1544796282.html\r\n\"Exploratory Data Analysis with R\" Roger D. Peng (ver 2015-07-03)\r\nhttp://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf\r\nhttp://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/\r\nJournal of Statistical Software November 2008, Volume 28, Issue 5.\r\nhttp://topepo.github.io/caret/training.html\r\nhttp://will-stanton.com/machine-learning-with-r-an-irresponsibly-fast-tutorial/\r\nPredictive Analytics with Microsoft Azure Machine Learning 2nd Edition: Edition 2\r\n\r\n## Appendix A ##\r\nVariables original names:\r\n#####\r\n\r\n    X; user_name; raw_timestamp_part_1; raw_timestamp_part_2; cvtd_timestamp; new_window; num_window; roll_belt;\r\n    pitch_belt; yaw_belt; total_accel_belt; kurtosis_roll_belt; kurtosis_picth_belt; kurtosis_yaw_belt; skewness_roll_belt;\r\n    skewness_roll_belt.1; skewness_yaw_belt; max_roll_belt; max_picth_belt; max_yaw_belt; min_roll_belt; min_pitch_belt;\r\n    min_yaw_belt; amplitude_roll_belt; amplitude_pitch_belt; amplitude_yaw_belt; var_total_accel_belt; avg_roll_belt;\r\n    stddev_roll_belt; var_roll_belt; avg_pitch_belt; stddev_pitch_belt; var_pitch_belt; avg_yaw_belt; stddev_yaw_belt;\r\n    var_yaw_belt; gyros_belt_x; gyros_belt_y; gyros_belt_z; accel_belt_x; accel_belt_y; accel_belt_z; magnet_belt_x;\r\n    magnet_belt_y; magnet_belt_z; roll_arm; pitch_arm; yaw_arm; total_accel_arm; var_accel_arm; avg_roll_arm; stddev_roll_arm;\r\n    var_roll_arm; avg_pitch_arm; stddev_pitch_arm; var_pitch_arm; avg_yaw_arm; stddev_yaw_arm; var_yaw_arm; gyros_arm_x;\r\n    gyros_arm_y; gyros_arm_z; accel_arm_x; accel_arm_y; accel_arm_z; magnet_arm_x; magnet_arm_y; magnet_arm_z;\r\n    kurtosis_roll_arm; kurtosis_picth_arm; kurtosis_yaw_arm; skewness_roll_arm; skewness_pitch_arm; skewness_yaw_arm;\r\n    max_roll_arm; max_picth_arm; max_yaw_arm; min_roll_arm; min_pitch_arm; min_yaw_arm; amplitude_roll_arm;\r\n    amplitude_pitch_arm; amplitude_yaw_arm; roll_dumbbell; pitch_dumbbell; yaw_dumbbell; kurtosis_roll_dumbbell;\r\n    kurtosis_picth_dumbbell; kurtosis_yaw_dumbbell; skewness_roll_dumbbell; skewness_pitch_dumbbell; skewness_yaw_dumbbell;\r\n    max_roll_dumbbell; max_picth_dumbbell; max_yaw_dumbbell; min_roll_dumbbell; min_pitch_dumbbell; min_yaw_dumbbell;\r\n    amplitude_roll_dumbbell; amplitude_pitch_dumbbell; amplitude_yaw_dumbbell; total_accel_dumbbell; var_accel_dumbbell;\r\n    avg_roll_dumbbell; stddev_roll_dumbbell; var_roll_dumbbell; avg_pitch_dumbbell; stddev_pitch_dumbbell; var_pitch_dumbbell;\r\n    avg_yaw_dumbbell; stddev_yaw_dumbbell; var_yaw_dumbbell; gyros_dumbbell_x; gyros_dumbbell_y; gyros_dumbbell_z;\r\n    accel_dumbbell_x; accel_dumbbell_y; accel_dumbbell_z; magnet_dumbbell_x; magnet_dumbbell_y; magnet_dumbbell_z;\r\n    roll_forearm; pitch_forearm; yaw_forearm; kurtosis_roll_forearm; kurtosis_picth_forearm; kurtosis_yaw_forearm;\r\n    skewness_roll_forearm; skewness_pitch_forearm; skewness_yaw_forearm;max_roll_forearm; max_picth_forearm; max_yaw_forearm;\r\n    min_roll_forearm; min_pitch_forearm; min_yaw_forearm; amplitude_roll_forearm; amplitude_pitch_forearm;\r\n    amplitude_yaw_forearm; total_accel_forearm; var_accel_forearm; avg_roll_forearm; stddev_roll_forearm; var_roll_forearm;\r\n    avg_pitch_forearm; stddev_pitch_forearm; var_pitch_forearm; avg_yaw_forearm; stddev_yaw_forearm; var_yaw_forearm;\r\n    gyros_forearm_x; gyros_forearm_y; gyros_forearm_z; accel_forearm_x; accel_forearm_y; accel_forearm_z; magnet_forearm_x;\r\n    magnet_forearm_y; magnet_forearm_z; classe;\r\n\r\nVariables names in NewTidyData:\r\n\r\n    X;UserName;RawTimestampPart_1;RawTimestampPart_2;CvtdTimestamp\r\n    NewWindow;NumWindow;RollBelt;PitchBelt;YawBelt\r\n    TotalAccelBelt;KurtosisRollBelt;KurtosisPicthBelt;KurtosisYawBelt;SkewnessRollBelt\r\n    SkewnessRollBelt.1;SkewnessYawBelt;MaxRollBelt;MaxPicthBelt;MaxYawBelt\r\n    MinRollBelt;MinPitchBelt;MinYawBelt;AmplitudeRollBelt;AmplitudePitchBelt\r\n    AmplitudeYawBelt;VarTotalAccelBelt;AvgRollBelt;StddevRollBelt;VarRollBelt\r\n    AvgPitchBelt;StddevPitchBelt;VarPitchBelt;AvgYawBelt;StddevYawBelt\r\n    VarYawBelt;GyrosBeltX;GyrosBeltY;GyrosBeltZ;AccelBeltX\r\n    AccelBeltY;AccelBeltZ;MagnetBeltX;MagnetBeltY;MagnetBeltZ\r\n    RollArm;PitchArm;YawArm;TotalAccelArm;VarAccelArm\r\n    AvgRollArm;StddevRollArm;VarRollArm;AvgPitchArm;StddevPitchArm\r\n    VarPitchArm;AvgYawArm;StddevYawArm;VarYawArm;GyrosArmX\r\n    GyrosArmY;GyrosArmZ;AccelArmX;AccelArmY;AccelArmZ\r\n    MagnetArmX;MagnetArmY;MagnetArmZ;KurtosisRollArm;KurtosisPicthArm\r\n    KurtosisYawArm;SkewnessRollArm;SkewnessPitchArm;SkewnessYawArm;MaxRollArm\r\n    MaxPicthArm;MaxYawArm;MinRollArm;MinPitchArm;MinYawArm\r\n    AmplitudeRollArm;AmplitudePitchArm;AmplitudeYawArm;RollDumbbell;PitchDumbbell\r\n    YawDumbbell;KurtosisRollDumbbell;KurtosisPicthDumbbell;KurtosisYawDumbbell;SkewnessRollDumbbell\r\n    SkewnessPitchDumbbell;SkewnessYawDumbbell;MaxRollDumbbell;MaxPicthDumbbell;MaxYawDumbbell\r\n    MinRollDumbbell;MinPitchDumbbell;MinYawDumbbell;AmplitudeRollDumbbell;AmplitudePitchDumbbell\r\n    AmplitudeYawDumbbell;TotalAccelDumbbell;VarAccelDumbbell;AvgRollDumbbell;StddevRollDumbbell\r\n    VarRollDumbbell;AvgPitchDumbbell;StddevPitchDumbbell;VarPitchDumbbell;AvgYawDumbbell\r\n    StddevYawDumbbell;VarYawDumbbell;GyrosDumbbellX;GyrosDumbbellY;GyrosDumbbellZ\r\n    AccelDumbbellX;AccelDumbbellY;AccelDumbbellZ;MagnetDumbbellX;MagnetDumbbellY\r\n    MagnetDumbbellZ;RollForearm;PitchForearm;YawForearm;KurtosisRollForearm\r\n    KurtosisPicthForearm;KurtosisYawForearm;SkewnessRollForearm;SkewnessPitchForearm;SkewnessYawForearm\r\n    MaxRollForearm;MaxPicthForearm;MaxYawForearm;MinRollForearm;MinPitchForearm\r\n    MinYawForearm;AmplitudeRollForearm;AmplitudePitchForearm;AmplitudeYawForearm;TotalAccelForearm\r\n    VarAccelForearm;AvgRollForearm;StddevRollForearm;VarRollForearm;AvgPitchForearm\r\n    StddevPitchForearm;VarPitchForearm;AvgYawForearm;StddevYawForearm;VarYawForearm\r\n    GyrosForearmX;GyrosForearmY;GyrosForearmZ;AccelForearmX;AccelForearmY\r\n    AccelForearmZ;MagnetForearmX;MagnetForearmY;MagnetForearmZ;Classe\r\n\r\n\r\n\r\n## Appendix B ##\r\n\r\n\r\n## Appendix C ##\r\n\r\n    library(\"dplyr\")\r\n\r\n#####   if you don't have the package use: install.packages(\"dplyr\")\r\n\r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-training.csv\") \r\n    pml.training <- read.csv(\"pml-training.csv\",header=TRUE, na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n    \r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-testing.csv\") \r\n    pml.testing <- read.csv(\"pml-testing.csv\",header=TRUE)\r\n\r\n#### Cleaning the non-alphanumeric characters, and using CamelCase structure\r\n\r\ntemp=names(pml.training)\r\ntemp=gsub(\"^a\",\"A\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^g\",\"G\",temp);\r\ntemp=gsub(\"^k\",\"K\",temp); temp=gsub(\"^m\",\"M\",temp); temp=gsub(\"^n\",\"N\",temp); temp=gsub(\"^p\",\"P\",temp);\r\ntemp=gsub(\"^r\",\"R\",temp); temp=gsub(\"^s\",\"S\",temp); temp=gsub(\"^t\",\"T\",temp); temp=gsub(\"^u\",\"U\",temp);\r\ntemp=gsub(\"^v\",\"V\",temp); temp=gsub(\"^y\",\"Y\",temp); temp=gsub(\"_a\",\"A\",temp); temp=gsub(\"_b\",\"B\",temp);\r\ntemp=gsub(\"_c\",\"C\",temp); temp=gsub(\"_d\",\"D\",temp); temp=gsub(\"_f\",\"F\",temp); temp=gsub(\"_g\",\"G\",temp);\r\ntemp=gsub(\"_k\",\"K\",temp); temp=gsub(\"_m\",\"M\",temp); temp=gsub(\"_n\",\"N\",temp); temp=gsub(\"_p\",\"P\",temp);\r\ntemp=gsub(\"_r\",\"R\",temp); temp=gsub(\"_s\",\"S\",temp); temp=gsub(\"_t\",\"T\",temp); temp=gsub(\"_u\",\"U\",temp);\r\ntemp=gsub(\"_v\",\"V\",temp); temp=gsub(\"_x\",\"X\",temp); temp=gsub(\"_y\",\"Y\",temp); temp=gsub(\"_w\",\"W\",temp);\r\ntemp=gsub(\"_z\",\"Z\",temp); temp=gsub(\"_1\",\"1\",temp); temp=gsub(\"_2\",\"2\",temp);\r\nTidyData = pml.training\r\ncolnames(TidyData) = temp\r\n\r\n####*removing variables that contain more than 95% of 'NA'*\r\nNewTidyData1 = TidyData[, colSums(is.na(TidyData))/nrow(TidyData) < 0.95]\r\ndim(NewTidyData1)\r\n\r\n####*removing observables that contain \"NA\"\r\ntemp = TidyData[, colSums(is.na(TidyData))/nrow(TidyData) > 0.95]\r\nNewTidyData2 = temp[, colSums(is.na(temp))/nrow(temp) != 1]\r\nNewTidyData2 = na.omit(NewTidyData2)\r\ndim(NewTidyData2)\r\n\r\n####*removed variables that only have NA:*\r\nsetdiff(names(temp),names(NewTidyData2))\r\n\r\n####*remained variables:*\r\nstr(NewTidayData)\r\n\r\n####*with the selected features*\r\n\r\n\r\ntemp=select(NewTidyData,UserName,Classe,starts_with('Var'),starts_with('Avg'))\r\nplot(temp[,c(19:30)], colour=temp$Classe,rm.na=TRUE)\r\n\r\nsplom( ~ temp[,19:30] | temp$UserName, cex=0.8, pch=16,tick.labels=FALSE)\r\nlevels(temp$Classe) <- c(\"red\",\"blue\",\"green\",\"black\",\"orange\")\r\n\r\npairs(temp[,19:30], cex=0.8, pch=21, bg =c(\"black\",\"red\", \"green3\", \"blue\",\"orange\")[unclass(temp$Classe)], oma=c(8,3,3,3))\r\npar(xpd = TRUE)\r\nlegend(\"bottom\", fill = unique(temp$Classe), legend = c(levels(temp$Classe)), horiz=TRUE, bty='n')\r\n\r\nfeaturePlot(x=temp[,19:30], y=temp$Classe, plot=\"pairs\", auto.key=list(columns=5))\r\n\r\n(Takes long time)\r\nsplom( ~ temp[,19:30] | temp$UserName, cex=0.7, groups=as.factor(temp$Classe),type=\"p\", pch=16, scales=list(x=list(at=NULL)), auto.key=list(space=\"right\", columns=1, points=FALSE, rectangles=TRUE, cex.title=1))\r\n \r\n\r\nNext, after splitting the data into training and testing sets and using the caret package to automate training and testing both random forest and partial least squares models using repeated 10-fold cross-validation (see the code), it turns out random forest outperforms PLS in this case, and performs fairly well overall:\r\n\r\n\r\n####*Verify if there is any \"NA\". If there is it can use na.rm = TRUE in mean()*\r\n    sum(is.na(NewTidyData))\r\n\r\ntemp2=select(NewTidyData,UserName,Classe,starts_with('Var'),starts_with('Avg'))\r\n\r\nlibrary(\"caret\")\r\nset.seed(123)\r\ninTrain <- createDataPartition(NewTidyData1$Class, p = 0.6, list = FALSE)\r\ntrainTidyData <- NewTidyData1[inTrain,]\r\ntestTidyData <- NewTidyData1[-inTrain,]\r\n\r\nmodelFit = train(Classe ~., data=trainTidyData, method=\"rf\", prox=TRUE)\r\nmodelfFit\r\n\r\ntest = test[apply(test, 1, function(x) !any(x == '#DIV/0!')), ] \r\n\r\n\r\nimpVar = as.data.frame(varImp(modelFit2, scale=TRUE)[1])\r\na = cbind(Variables = rownames(impVar),impVar)\r\nimpVarOrd = arrange(a,desc(Overall))\r\nwrite.csv(b,\"varImp_modelFit2.csv\")\r\n\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/anonymous-1618/ML/master/Rplot19.png\">\r\n  <b>Figure 1 - </b>Variable Importance</b><br>\r\n  </p>\r\n\r\n![alt text](https://raw.githubusercontent.com/anonymous-1618/ML/master/Rplot19.png \"test\")\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}