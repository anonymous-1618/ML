{
  "name": "Practical Machine Learning Course",
  "tagline": "Project Assigment",
  "body": "# How well are we doing our Physical exercise?\r\n***************************************************\r\n\r\n###Summary:\r\nNowadays, to improve the quality of life  more and more people are practising sports.Practice not only improves the body's physical condition or fitness but also decreases the likelihood of many ills related to a sedentary life style, such as heart disease. But, it is not only the act or quantity of doing physical exercise that matters. Many people usually forget that physical exercise when done in the wrong way, may be less effective. But on the other hand they can also be very damaging causing undesirable injuries. Therefore, the right way to perform the exercise is considered a top priority.\r\nThis project is based on the work made by Veloso *et al.* \"Qualitative Activity Recognition of Weight Lifting Exercises\". The authors mounted sensors in six male volunteers to lift a relatively light dumbbell (1.25kg) weight. Five different ways (where only one is correct) to perform the lifting exercise were monitored by the sensors. The data collected were analysed and a machine learning model was built to assess the correctness and feedback to the user in real-time; increasing the likelihood of the exercise effectiveness. \r\n\r\n###Scope:\r\nAs part of the Coursera assessment, the report described here are restricted to answer the followings:\r\n\r\n*   Predict the manner in which the exercise was done.\r\n*   Show how cross validation was implemented\r\n*   Analyse the sample error.\r\n*   Discuss the assumptions made.\r\n*   Apply the proposed model to predict 20 different test cases.\r\n\r\n###Experiment description: \r\nSix (6) volunteers wore four (4) \"9 Degrees of Freedom - Razor IMU\". Each one of the Razor IMU is composed of three sensors: accelerometer, gyroscope and magnetometer, each one providing 3 degrees of freedom. Therefore, a total of 9 degrees of freedom per location (see the sketch). The four locations were: glove, armband, lumbar belt and dumbbell.\r\n\r\n![](https://raw.githubusercontent.com/anonymous-1618/ML/master/sketch.png)\r\n\r\nThe volunteers were male participants aged between 20-28 years. They performed one set of 10 repetitions of the activity \"Unilateral Dumbbell Biceps Curl\" in five different \"classes\", one class is the right way (class A below) and the others the wrong way (classes B to E below):\r\n- Class A - The right exercise (*i.e*., exactly according to the specification)\r\n- Class B - Throwing the elbows to the front\r\n- Class C - Lifting the dumbbell only halfway \r\n- Class D - Lowering the dumbbell only halfway\r\n- Class E - Throwing the hips to the front\r\n\r\n###Exploratory Data analysis:\r\n\r\n**1.\tDownload the data from a provided URL.**\r\n\r\n```R\r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-training.csv\")\r\n    \r\n    fileUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n    download.file(fileUrl, destfile=\"./pml-testing.csv\")\r\n```\r\n\r\n**2.\tLoad to R Global environment the files from their respective folders.**\r\n\r\n```R\r\n    pml.training <- read.csv(\"pml-training.csv\",header=TRUE, na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n    pml.testing  <- read.csv(\"pml-testing.csv\", header=TRUE, na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n```\r\n**3.\tTake a look at the data and analyse the data structure**\r\n\r\n```R\r\nstr(pml.training, list.len=20)\r\n```\r\n\r\n```R\r\n'data.frame':\t19622 obs. of  160 variables:\r\n $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ user_name               : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...\r\n $ raw_timestamp_part_2    : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...\r\n $ cvtd_timestamp          : Factor w/ 20 levels \"02/12/2011 13:32\",..: 9 9 9 9 9 9 9 9 9 9 ...\r\n $ new_window              : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...\r\n $ roll_belt               : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...\r\n $ pitch_belt              : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...\r\n $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...\r\n $ total_accel_belt        : int  3 3 3 3 3 3 3 3 3 3 ...\r\n $ kurtosis_roll_belt      : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ kurtosis_picth_belt     : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ kurtosis_yaw_belt       : logi  NA NA NA NA NA NA ...\r\n $ skewness_roll_belt      : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ skewness_roll_belt.1    : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ skewness_yaw_belt       : logi  NA NA NA NA NA NA ...\r\n $ max_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...\r\n $ max_picth_belt          : int  NA NA NA NA NA NA NA NA NA NA ...\r\n $ max_yaw_belt            : num  NA NA NA NA NA NA NA NA NA NA ...\r\n  [list output truncated]\r\n```\r\n\r\nRaw data variables for each sensor per location are recorded as follows:\r\n*   Triaxial accelerometer:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n*   Triaxial gyroscope:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n*   Triaxial  magnetometer:\r\n    -   3 (x,y,z) x 4 (locations) = 12  \r\n\r\nA total of 36 raw data variables were then recorded.\r\n\r\n\"Fusion\" variables obtained from the raw data variables were also recorded:\r\n*   Three Rotational angles (roll, pitch, yaw) per location:\r\n    -   3  x 4 (locations) = 12\r\n*   Total acceleration per location:\r\n    -   1  x 4 (locations) = 4\r\n\r\nThe data from accelerometer, magnetometer and gyroscope are \"fused\" by using a Direction Cosine Matrix (DCM) algorithm in the Razor IMU, that means a total of 16 \"fused\" variables are recorded. Furthermore, in some of the \"fused\" variables, the following *descriptive statistics* variables (\"stat\") were recorded:\r\n*   Max, Min, standard deviation(stddev), variance(var), average(avg) of rotational angles:\r\n    -   5 x 12 = 60      \r\n*   Amplitude of rotational angles (amplitude):\r\n    -   3 x 4  = 12\r\n*   kurtosis and skewness of rotational angles:\r\n    -   2 x 12 = 24\r\n*   variance(var) of total acceleration:\r\n    -   1 x 4  = 4\r\n\r\nA total of 100 \"stat\" variables.\r\n\r\nThe remainder variables:\r\n*   Volunteer ID (user_name)\r\n*   Exercise type (class)\r\n*   Index or observable number (X)\r\n*   time & date (cvtd_timestamp)\r\n*   Four (4) time variables (raw_timestample_part_1 & raw_timestample_part_1, new_window, num_window);\r\n\r\nA total of 8 remainder variables.\r\nIn summary, the experiment recorded a total number of 160 (36 + 16 + 100 + 8) variables.\r\n\r\nEach individual performed 5 \"classes\" of exercises. Each exercise was repeated 10 times. All together a total of 19642 recorded observables were derived, which were then split by Coursera into two files:\r\n*  train set to create the Model loaded as \"pml.training\" is a dataframe of 19622 x 160 \r\n*  test set to answer the Quiz loaded as \"pml.testing\"  is a dataframe of 20 x 160\r\n\r\n**4.  Data Cleaning**\r\n\r\nThe data was cleaned and the variables re-labelled by using *CamelCase* structure.\r\n```R\r\n    temp=names(pml.training)\r\n    temp=gsub(\"^a\",\"A\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^g\",\"G\",temp); \r\n    temp=gsub(\"^k\",\"K\",temp); temp=gsub(\"^m\",\"M\",temp); temp=gsub(\"^n\",\"N\",temp); temp=gsub(\"^p\",\"P\",temp);\r\n    temp=gsub(\"^r\",\"R\",temp); temp=gsub(\"^s\",\"S\",temp); temp=gsub(\"^t\",\"T\",temp); temp=gsub(\"^u\",\"U\",temp);\r\n    temp=gsub(\"^v\",\"V\",temp); temp=gsub(\"^y\",\"Y\",temp); temp=gsub(\"_a\",\"A\",temp); temp=gsub(\"_b\",\"B\",temp);\r\n    temp=gsub(\"_c\",\"C\",temp); temp=gsub(\"_d\",\"D\",temp); temp=gsub(\"_f\",\"F\",temp); temp=gsub(\"_g\",\"G\",temp);\r\n    temp=gsub(\"_k\",\"K\",temp); temp=gsub(\"_m\",\"M\",temp); temp=gsub(\"_n\",\"N\",temp); temp=gsub(\"_p\",\"P\",temp);\r\n    temp=gsub(\"_r\",\"R\",temp); temp=gsub(\"_s\",\"S\",temp); temp=gsub(\"_t\",\"T\",temp); temp=gsub(\"_u\",\"U\",temp);\r\n    temp=gsub(\"_v\",\"V\",temp); temp=gsub(\"_x\",\"X\",temp); temp=gsub(\"_y\",\"Y\",temp); temp=gsub(\"_w\",\"W\",temp);\r\n    temp=gsub(\"_z\",\"Z\",temp); temp=gsub(\"_1\",\"1\",temp); temp=gsub(\"_2\",\"2\",temp);\r\n    TidyData = pml.training\r\n    colnames(TidyData) = temp\r\n```\r\nAlso, from the data structure we observe that there are missing values (\"NA\") in the data. We can check if the proportion is relevant using\r\n```R\r\n    mean(is.na(TidyData))\r\n```\r\n```R\r\n    [1] 0.6131835\r\n```\r\nOver 61% of the data is missing, which is a very significant proportion. So, we can set a threshold to remove variables that contain more than 95% of \"NA\", for example:\r\n```R\r\n    NewTidyData1 = TidyData[, colSums(is.na(TidyData))/nrow(TidyData) < 0.95]\r\n    dim(NewTidyData1)\r\n``` \r\n```R\r\n   [1] 19622    60\r\n```\r\n```R\r\n   mean(is.na(NewTidyData1))\r\n``` \r\n```R\r\n   [1] 0\r\n```\r\nThe `NewTidyData1` now does not contain any missing values. The first 20 rows of data structure of the `NewTidyData1`:\r\n```R\r\n str(NewTidyData1, list.len=20)\r\n``` \r\n```R\r\n'data.frame':\t19622 obs. of  60 variables:\r\n $ X                 : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ UserName          : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n $ RawTimestampPart1 : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...\r\n $ RawTimestampPart2 : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...\r\n $ CvtdTimestamp     : Factor w/ 20 levels \"02/12/2011 13:32\",..: 9 9 9 9 9 9 9 9 9 9 ...\r\n $ NewWindow         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ NumWindow         : int  11 11 11 12 12 12 12 12 12 12 ...\r\n $ RollBelt          : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...\r\n $ PitchBelt         : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...\r\n $ YawBelt           : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...\r\n $ TotalAccelBelt    : int  3 3 3 3 3 3 3 3 3 3 ...\r\n $ GyrosBeltX        : num  0 0.02 0 0.02 0.02 0.02 0.02 0.02 0.02 0.03 ...\r\n $ GyrosBeltY        : num  0 0 0 0 0.02 0 0 0 0 0 ...\r\n $ GyrosBeltZ        : num  -0.02 -0.02 -0.02 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02 0 ...\r\n $ AccelBeltX        : int  -21 -22 -20 -22 -21 -21 -22 -22 -20 -21 ...\r\n $ AccelBeltY        : int  4 4 5 3 2 4 3 4 2 4 ...\r\n $ AccelBeltZ        : int  22 22 23 21 24 21 21 21 24 22 ...\r\n $ MagnetBeltX       : int  -3 -7 -2 -6 -6 0 -4 -2 1 -3 ...\r\n $ MagnetBeltY       : int  599 608 600 604 600 603 599 603 602 609 ...\r\n $ MagnetBeltZ       : int  -313 -311 -305 -310 -302 -312 -311 -313 -312 -308 ...\r\n  [list output truncated]    \r\n```\r\nFrom the initial 160, there are a total of 60 variables. We can still reduce the number of variables **assuming** that the prediction is not time and individual dependent, that is, independent of the person and the time that the exercise was performed. Then, variables such as: *\"UserName\", \"RawTimestampPart1\", \"RawTimestampPart2\", \"CvtdTimestamp\", \"NewWindow\" and \"NumWindow\"* can be removed together with the index *\"X\"* variable:\r\n```R\r\n    library(dplyr)\r\n    SelectData = select(NewTidyData1,-c(X:NumWindow))\r\n    dim(SelectData)\r\n```\r\n```R    \r\n    [1] 19622    53\r\n```\r\nTherefore, the number of variables is reduced to 53.\r\n\r\n###Model Building\r\n\r\n**5.\tPropose Machine learning models base on the exploratory data**\r\n\r\nI used the *caret* package in this work. Caret stands for Classification And REgression Training. It is a great toolkit to build  classification and regression models. Caret also provides the means for: Data preparation, Data splitting, Training a Model, Model evaluation, and  Variable selection.\r\n\r\n**5.1.    Data Preparation - Removing redundant variables by a correlation matrix**\r\n\r\nThe data variables may be correlated to each other, which it may lead to redundancy in the model (*_assumption_*). By using `findCorrelation` from the *caret* package, we can obtain the correlation matrix between the data variables. The function \r\ncan plot the data set to visualise those correlations (See Figure 1).  The plot makes it less difficult to choose the threshold of the correlation coefficient in  order to remove redundant variables. I chose an absolute value for correlation coefficient of 0.90 as the threshold. Seven (7) other variables can be dropped.\r\n```R\r\n    library(caret)\r\n    library(corrplot)\r\n    threshold   <-  0.90\r\n    corMatrix   <-  cor(SelectData[,1:52])\r\n    corrplot(corMatrix, method=\"square\", order=\"hclust\") # Plot the correlation matrix\r\n    \r\n    highCor <-  findCorrelation(corMatrix, threshold) #sub set those that thas correlation >, = 0.9, or < -0.9\r\n    highCorRm   <-  row.names(corMatrix)[highCor]\r\n    highCorRm\r\n``` \r\n```R\r\n    [1] \"AccelBeltZ\"    \"RollBelt\"       \"AccelBeltY\"     \"AccelBeltX\"     \r\n        \"GyrosDumbbellX\" \"GyrosDumbbellZ\"  \"GyrosArmX\"\r\n``` \r\n```R\r\n    SelectData2 <- SelectData[, -highCor]\r\n    dim(SelectData2)\r\n``` \r\n```R\r\n    [1] 19622    46\r\n```\r\n<p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/anonymous-1618/ML/master/Fig1.png\">\r\n  <b>Figure 1 - </b>Variables Correlation Map</b><br>\r\n  </p>\r\n\r\n**5.2.  Data splitting**\r\n\r\nThe *caret* function `createDataPartition` is used to randomly split the data set. I set the standard proportion of 60% of the data to be used for model training and 40% used for testing model performance.\r\n```R\r\n    library(\"caret\")\r\n    library(\"e1071\")\r\n    set.seed(1023)\r\n    inTrain <- createDataPartition(SelectData2$Class, p = 0.6, list = FALSE)\r\n    trainData <- SelectData2[inTrain,]\r\n    testData <- SelectData2[-inTrain,]\r\n```\r\n**5.3.  Training a Model/Tuning Parameters/Variable Selection**\r\n\r\nAs the main question of this assignment is about classification, I choose Random Forest (\"rf\") to build the model. Tuning the model means to choose a set of parameters to be evaluated. Once the model and tuning parameters are chosen, the type of resampling needs to be opted for. Caret Package has tools to perform, k-fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap (**default**) resampling. Once the resampling was processed, the caret `train` function automatically chooses the best tuning parameters associated to the model.\r\n\r\n```R\r\n    set.seed(3023)\r\n    modelFit2 = train(Classe ~., data=trainData, method=\"rf\", prox=TRUE)\r\n    modelFit2\r\n```\r\n\r\n```R\r\n    11776 samples\r\n    45 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\n    No pre-processing\r\n    Resampling: Bootstrapped (25 reps) \r\n    Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... \r\n    Resampling results across tuning parameters:\r\n\r\n    mtry  Accuracy   Kappa    \r\n    2     0.9856378  0.9818241\r\n    23    0.9874277  0.9840903\r\n    45    0.9752460  0.9686771\r\n\r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 23. \r\n```\r\nAccuracy is the default metric selected to be optimized in `train()` for categorical variables, in this case, \"Classe\". The proposed model shows that 23 predictors were needed to get the best accuracy on the training set. In order words, `caret` has optimised the model and concluded that selecting 23 most important predictors gives the best accuracy. The \"out-of-bag\" (OOB) error rate of the final model can be obtained by:\r\n\r\n```R\r\nmodelFit2$finalModel\r\n``` \r\n\r\n```R\r\n    Call:\r\n    randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) \r\n               Type of random forest: classification\r\n                     Number of trees: 500\r\n    No. of variables tried at each split: 23\r\n\r\n            OOB estimate of  error rate: 0.91%\r\n    Confusion matrix:\r\n         A    B    C    D    E    class.error\r\n    A 3338    6    1    1    2  0.002986858\r\n    B   22 2243   14    0    0  0.015796402\r\n    C    0   14 2031    9    0  0.011197663\r\n    D    1    0   25 1903    1  0.013989637\r\n    E    0    1    2    8 2154  0.005080831\r\n```\r\n\r\nHowever the better error estimates is obtained by using the testing set (as I show in the Evaluation section). We can also calculate the variable importance in the model by using the caret package `varImp` function:\r\n\r\n```R\r\n    varImp(modelFit2, scale=TRUE)[[1]]\r\n```\r\n```R\r\n                        Overall\r\n    PitchBelt           71.88966676\r\n    YawBelt            100.00000000\r\n    TotalAccelBelt      15.13943598\r\n    GyrosBeltX           4.10645743\r\n    GyrosBeltY           4.29909507\r\n    GyrosBeltZ          28.77409715\r\n    MagnetBeltX         19.06090724\r\n    MagnetBeltY         46.42143718\r\n    MagnetBeltZ         29.86221750\r\n    RollArm             13.36456325\r\n    PitchArm             5.30347638\r\n    YawArm              14.18155500\r\n    TotalAccelArm        2.06975968\r\n    GyrosArmY            7.26378887\r\n    GyrosArmZ            0.00000000\r\n    AccelArmX            6.52209427\r\n    AccelArmY            6.87352864\r\n    AccelArmZ            2.89186020\r\n    MagnetArmX           8.30696563\r\n    MagnetArmY           9.05507339\r\n    MagnetArmZ           7.15700321\r\n    RollDumbbell        27.13032649\r\n    PitchDumbbell        6.15535378\r\n    YawDumbbell         11.93242077\r\n    TotalAccelDumbbell  18.91942501\r\n    GyrosDumbbellY      10.63841552\r\n    AccelDumbbellX       7.25232796\r\n    AccelDumbbellY      27.33024178\r\n    AccelDumbbellZ      19.02170061\r\n    MagnetDumbbellX     28.23072724\r\n    MagnetDumbbellY     51.95465439\r\n    MagnetDumbbellZ     63.81343801\r\n    RollForearm         50.97284067\r\n    PitchForearm        86.88962083\r\n    YawForearm           7.33542669\r\n    TotalAccelForearm    2.41288158\r\n    GyrosForearmX        0.02460776\r\n    GyrosForearmY        3.64303343\r\n    GyrosForearmZ        0.95471639\r\n    AccelForearmX       21.54543983\r\n    AccelForearmY        3.55059378\r\n    AccelForearmZ       17.05528477\r\n    MagnetForearmX       9.02319380\r\n    MagnetForearmY       7.14137478\r\n    MagnetForearmZ      15.57428210\r\n```\r\nWe can also get the plot of the variable importance sorted in descending order by `plot(varImp(modelFit2, scale=TRUE))`. See Figure 2.\r\n<p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/anonymous-1618/ML/master/Fig2.png\">\r\n  <b>Figure 2 - </b>Variable Importance</b><br>\r\n  </p>\r\n\r\n**5.4.    Model Evaluation**\r\nThe evalution of the training fit model can be made by using the caret `predict()`function on the testing set:\r\n```R\r\n    testPred <- predict(modelFit2, newdata = testData)\r\n```\r\nNow we can compare the prediction result of the testing set with the actual values by using the `confusionMatrix()` function:\r\n```R\r\n    confusionMatrix(testData$Classe,testPred)\r\n```\r\n\r\n```R\r\n    Confusion Matrix and Statistics\r\n\r\n              Reference\r\n    Prediction     A    B    C    D    E\r\n         A      2232    0    0    0    0\r\n         B         5 1505    8    0    0\r\n         C         0    5 1361    2    0\r\n         D         0    1   22 1261    2\r\n         E         0    1    3    4 1434\r\n\r\n    Overall Statistics\r\n                                          \r\n               Accuracy : 0.9932          \r\n                 95% CI : (0.9912, 0.9949)\r\n    No Information Rate : 0.2851          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9915          \r\n    Mcnemar's Test P-Value : NA              \r\n\r\n    Statistics by Class:\r\n\r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9978   0.9954   0.9763   0.9953   0.9986\r\n    Specificity            1.0000   0.9979   0.9989   0.9962   0.9988\r\n    Pos Pred Value         1.0000   0.9914   0.9949   0.9806   0.9945\r\n    Neg Pred Value         0.9991   0.9989   0.9949   0.9991   0.9997\r\n    Prevalence             0.2851   0.1927   0.1777   0.1615   0.1830\r\n    Detection Rate         0.2845   0.1918   0.1735   0.1607   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9989   0.9967   0.9876   0.9957   0.9987\r\n```\r\nTherefore the accuray estimated for the model fit is 0.9932 which means that the estimation error is ca. 0.68%.\r\n\r\n###Model Parameters Effect:\r\n1.Selecting variables by Variable importance ranking:\r\n\r\nI tried to improve the model by changing the number of predictors. Based on the previous results, I reduced more of the number of variables by creating a cutoff point for the best variables by their importance:\r\n\r\n```R\r\n    library(dplyr)\r\n    temp =varImp(modelFit2, scale=TRUE)\r\n    temp.df =as.data.frame(temp[[1]])\r\n    temp.df = cbind(Variables = rownames(temp.df),temp.df)\r\n    Impvar.names = as.vector(filter(temp.df, Overall > 10)[,1])     #filtering to variables with importance > 10%#\r\n    Impvar.names\r\n```\r\n```R\r\n    [1] \"PitchBelt\"          \"YawBelt\"            \"TotalAccelBelt\"     \"GyrosBeltZ\"        \r\n    [5] \"MagnetBeltX\"        \"MagnetBeltY\"        \"MagnetBeltZ\"        \"RollArm\"           \r\n    [9] \"YawArm\"             \"RollDumbbell\"       \"YawDumbbell\"        \"TotalAccelDumbbell\"\r\n    [13] \"GyrosDumbbellY\"     \"AccelDumbbellY\"     \"AccelDumbbellZ\"     \"MagnetDumbbellX\"   \r\n    [17] \"MagnetDumbbellY\"    \"MagnetDumbbellZ\"    \"RollForearm\"        \"PitchForearm\"      \r\n    [21] \"AccelForearmX\"      \"AccelForearmZ\"      \"MagnetForearmZ\"  \r\n```\r\n```R\r\n    selectVar = c(Impvar.names,\"Classe\")\r\n    trainDataImp = trainData[,Impvar.names]\r\n    modelFit2a = train(Classe ~., data=trainDataImp, method=\"rf\", prox=TRUE)\r\n```\r\n```R\r\n    modelFit2a = train(Classe ~., data=trainDataImp, method=\"rf\", prox=TRUE)\r\n    modelFit2a\r\n```\r\n```R\r\n    Random Forest \r\n        \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n        \r\n    No pre-processing\r\n    Resampling: Bootstrapped (25 reps) \r\n    Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... \r\n    Resampling results across tuning parameters:\r\n        \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9815209  0.9765947\r\n      12    0.9816863  0.9768206\r\n      23    0.9697268  0.9616665\r\n        \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12.\r\n```\r\n```R\r\n   modelFit2a$finalModel\r\n```\r\n```R\r\n    Call:\r\n    randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 12\r\n        \r\n            OOB estimate of  error rate: 1.07%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3337    7    1    1    2 0.003285544\r\n    B   22 2230   24    1    2 0.021500658\r\n    C    0   15 2021   18    0 0.016066212\r\n    D    1    1   19 1907    2 0.011917098\r\n    E    0    1    3    6 2155 0.004618938\r\n```\r\n\r\n```R\r\n    testPred <- predict(modelFit2a, newdata = testData)\r\n    confusionMatrix(testData$Classe,testPred)\r\n```\r\n\r\n```R\r\n    Confusion Matrix and Statistics\r\n        \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2231    1    0    0    0\r\n             B    5 1499   13    1    0\r\n             C    0    4 1355    9    0\r\n             D    0    1   21 1260    4\r\n             E    0    0    3    5 1434\r\n        \r\n    Overall Statistics\r\n                                                  \r\n                   Accuracy : 0.9915          \r\n                     95% CI : (0.9892, 0.9934)\r\n        No Information Rate : 0.285           \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                                  \r\n                      Kappa : 0.9892          \r\n        Mcnemar's Test P-Value : NA              \r\n        \r\n        Statistics by Class:\r\n        \r\n                             Class: A Class: B Class: C Class: D Class: E\r\n        Sensitivity            0.9978   0.9960   0.9734   0.9882   0.9972\r\n        Specificity            0.9998   0.9970   0.9980   0.9960   0.9988\r\n        Pos Pred Value         0.9996   0.9875   0.9905   0.9798   0.9945\r\n        Neg Pred Value         0.9991   0.9991   0.9943   0.9977   0.9994\r\n        Prevalence             0.2850   0.1918   0.1774   0.1625   0.1833\r\n        Detection Rate         0.2843   0.1911   0.1727   0.1606   0.1828\r\n        Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n        Balanced Accuracy      0.9988   0.9965   0.9857   0.9921   0.9980\r\n```        \r\nIn this trial, the number of predictors selected in the final model was again decreased to **12 predictors**. This makes the model less complex. However, the OOB error rate (1.07%) and the test accuracy (0.9915) are slightly poorer than the original model. Also, the calculation time increased by half hour (ca. 5.5 h). \r\n\r\n\r\n2.Cross-Validation type:\r\n\r\nI also worked with 10-fold cross-validation (k=10). This resampling in general was much faster than bootstrap, that is, roughly ten-fold faster. As we can see in the results shown below, the accuracy is very similar to the \"rf\" but the OOB error is smaller that the original model. The following calculation uses the data with an importance variable cutoff of 10%.\r\n```R\r\n    set.seed(1825)\r\n    fitControl <- trainControl( method = \"cv\", number = 10)\r\n    modelFit2c <- train(Classe ~ ., data = trainDataImp, method=\"rf\", trControl = fitControl)\r\n    modelFit2c\r\n```\r\n\r\n```R\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       23 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10598, 10598, 10599, 10599, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9862432  0.9825951\r\n      12    0.9872615  0.9838864\r\n      23    0.9783446  0.9726068\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 12. \r\n```\r\n\r\n```R\r\n  modelFit2c$finalModel\r\n\r\n  Call:\r\n   randomForest(x = x, y = y, mtry = param$mtry) \r\n                 Type of random forest: classification\r\n                       Number of trees: 500\r\n  No. of variables tried at each split: 12\r\n  \r\n          OOB estimate of  error rate: 1.14%\r\n  Confusion matrix:\r\n       A    B    C    D    E class.error\r\n  A 3335    9    1    1    2 0.003882915\r\n  B   24 2231   24    0    0 0.021061869\r\n  C    0   21 2017   16    0 0.018013632\r\n  D    1    1   18 1908    2 0.011398964\r\n  E    0    1    4    9 2151 0.006466513\r\n```\r\n\r\n```R\r\n  testPred <- predict(modelFit2c, newdata = testData)\r\n  confusionMatrix(testData$Classe,testPred)\r\n```\r\n\r\n```R\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2231    1    0    0    0\r\n             B    7 1498   13    0    0\r\n             C    0    4 1354   10    0\r\n             D    0    2   18 1263    3\r\n             E    0    1    4    3 1434\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9916          \r\n                     95% CI : (0.9893, 0.9935)\r\n        No Information Rate : 0.2852          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9894          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9969   0.9947   0.9748   0.9898   0.9979\r\n    Specificity            0.9998   0.9968   0.9978   0.9965   0.9988\r\n    Pos Pred Value         0.9996   0.9868   0.9898   0.9821   0.9945\r\n    Neg Pred Value         0.9988   0.9987   0.9946   0.9980   0.9995\r\n    Prevalence             0.2852   0.1919   0.1770   0.1626   0.1832\r\n    Detection Rate         0.2843   0.1909   0.1726   0.1610   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9983   0.9958   0.9863   0.9932   0.9983\r\n```\r\nPerforming the same calculation but now using the original data set with 46 variables (no cutoff) and comparing with the original modelFit2, we can compare the effect of only 10-fold cross-validation (k=10).\r\n```R\r\n    set.seed(10025)\r\n    fitControl <- trainControl( method = \"cv\", number = 10)\r\n    modelFit2f <- train(Classe ~ ., data = trainData, method=\"rf\", trControl = fitControl)\r\n    modelFit2f\r\n```\r\n```R\r\n    Random Forest \r\n    \r\n    11776 samples\r\n       45 predictor\r\n        5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n    No pre-processing\r\n    Resampling: Cross-Validated (10 fold) \r\n    Summary of sample sizes: 10599, 10599, 10599, 10598, 10598, 10598, ... \r\n    Resampling results across tuning parameters:\r\n    \r\n      mtry  Accuracy   Kappa    \r\n       2    0.9889613  0.9860343\r\n      23    0.9898949  0.9872163\r\n      45    0.9791109  0.9735707\r\n    \r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 23. \r\n```\r\n\r\n```R\r\n    modelFit2f$finalModel\r\n``` \r\n\r\n```R\r\n    Call:\r\n     randomForest(x = x, y = y, mtry = param$mtry) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 23\r\n    \r\n            OOB estimate of  error rate: 0.86%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3340    5    1    0    2 0.002389486\r\n    B   23 2244   11    0    1 0.015357613\r\n    C    0   14 2031    9    0 0.011197663\r\n    D    1    1   23 1904    1 0.013471503\r\n    E    0    0    2    7 2156 0.004157044\r\n```\r\n\r\n```R\r\n    testPred <- predict(modelFit2f, newdata = testData)\r\n    confusionMatrix(testData$Classe,testPred)\r\n```\r\n\r\n```R\r\n    Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2232    0    0    0    0\r\n             B    6 1503    9    0    0\r\n             C    0    5 1362    1    0\r\n             D    0    0   24 1260    2\r\n             E    0    0    3    5 1434\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.993           \r\n                     95% CI : (0.9909, 0.9947)\r\n        No Information Rate : 0.2852          \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9911          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9967   0.9742   0.9953   0.9986\r\n    Specificity            1.0000   0.9976   0.9991   0.9960   0.9988\r\n    Pos Pred Value         1.0000   0.9901   0.9956   0.9798   0.9945\r\n    Neg Pred Value         0.9989   0.9992   0.9944   0.9991   0.9997\r\n    Prevalence             0.2852   0.1922   0.1782   0.1614   0.1830\r\n    Detection Rate         0.2845   0.1916   0.1736   0.1606   0.1828\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9987   0.9972   0.9867   0.9957   0.9987\r\n```\r\nThis result is very welcoming because the accuracy is very similar to modelFit2 but the OOB error rate is small (0.86% vs 0.91%). But the most relevant is the modelling time. The CV resampling reduced the time from 5 hours to 30 minutes!\r\n \r\n\r\n3.Change the number of trees\r\n\r\nWe can obtain the relationship model error of classification and number of trees in the \"Random Forest\" model. For instance, the model modelFit2c (10-fold cross-validation with importance variable cutoff 10%) has the error related to the number of trees shown in Figure 3:\r\n```R\r\n    library(reshape)\r\n    df.melted <- melt(modelFit2c$finalModel$err.rate, id = \"ntree\")\r\n    names(df.melted) = c('ntree','Classe','Error')\r\n    ggplot(data = df.melted, aes(x = ntree, y = Error, color = Classe)) \r\n    +   geom_line(size=1)\r\n```    \r\n <p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/anonymous-1618/ML/master/Fig3.png\">\r\n  <b>Figure 3 - </b>Random Forest Model Error per #tree</b><br>\r\n  </p>\r\n\r\nWe can see in Figure 3 that the error in all of them stabilised early than ntree=500 (default). So, by trying a model fit with ntree=200, we have:\r\n```R\r\n  set.seed(2825)\r\n  fitControl <- trainControl( method = \"cv\", number = 10)\r\n  modelFit2b <- train(Classe ~ ., data = trainDataImp, method=\"rf\", ntree=200, trControl = fitControl)\r\n  modelFit2b\r\n```\r\n```R\r\n  Random Forest \r\n    \r\n  11776 samples\r\n     23 predictor\r\n      5 classes: 'A', 'B', 'C', 'D', 'E' \r\n    \r\n  No pre-processing\r\n  Resampling: Cross-Validated (10 fold) \r\n  Summary of sample sizes: 10598, 10599, 10599, 10597, 10598, 10598, ... \r\n  Resampling results across tuning parameters:\r\n    \r\n    mtry  Accuracy   Kappa    \r\n     2    0.9866676  0.9831335\r\n    12    0.9871779  0.9837805\r\n    23    0.9774111  0.9714243\r\n    \r\n  Accuracy was used to select the optimal model using  the largest value.\r\n  The final value used for the model was mtry = 12. \r\n```\r\n\r\n```R\r\n  modelFit2b$finalModel\r\n```\r\n\r\n```R\r\n  Call:\r\n  randomForest(x = x, y = y, ntree = 200, mtry = param$mtry) \r\n               Type of random forest: classification\r\n                       Number of trees: 200\r\n  No. of variables tried at each split: 12\r\n  \r\n          OOB estimate of  error rate: 1.26%\r\n  Confusion matrix:\r\n       A    B    C    D    E class.error\r\n  A 3328   14    2    2    2 0.005973716\r\n  B   24 2230   23    1    1 0.021500658\r\n  C    0   20 2016   18    0 0.018500487\r\n  D    1    0   20 1902    7 0.014507772\r\n  E    0    0    3   10 2152 0.006004619\r\n```\r\n\r\n```R\r\n  testPred <- predict(modelFit2b, newdata = testData)\r\n  confusionMatrix(testData$Classe,testPred)\r\n```\r\n\r\n```R\r\n  Confusion Matrix and Statistics\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2230    2    0    0    0\r\n             B    6 1495   16    1    0\r\n             C    0    4 1354   10    0\r\n             D    0    0   20 1263    3\r\n             E    0    0    5    5 1432\r\n    \r\n    Overall Statistics\r\n                                              \r\n                   Accuracy : 0.9908          \r\n                     95% CI : (0.9885, 0.9928)\r\n        No Information Rate : 0.285           \r\n        P-Value [Acc > NIR] : < 2.2e-16       \r\n                                              \r\n                      Kappa : 0.9884          \r\n     Mcnemar's Test P-Value : NA              \r\n    \r\n    Statistics by Class:\r\n    \r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9973   0.9960   0.9706   0.9875   0.9979\r\n    Specificity            0.9996   0.9964   0.9978   0.9965   0.9984\r\n    Pos Pred Value         0.9991   0.9848   0.9898   0.9821   0.9931\r\n    Neg Pred Value         0.9989   0.9991   0.9937   0.9976   0.9995\r\n    Prevalence             0.2850   0.1913   0.1778   0.1630   0.1829\r\n    Detection Rate         0.2842   0.1905   0.1726   0.1610   0.1825\r\n    Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    Balanced Accuracy      0.9985   0.9962   0.9842   0.9920   0.9982\r\n```\r\n    \r\nThe accuracy did not change by much, but the OOB error rate was the highest of all the evaluated models (1.26%). On the other hand, it was the fitted model that took less time (~ 6 minutes). \r\n    \r\n###Predicting the 20 test cases:\r\nAfter tidying up the test data set (pml.testing), we can estimate the exercise Classe of each one of the 20 test cases.\r\n```R\r\n  temp=names(pml.testing)\r\n  temp=gsub(\"^a\",\"A\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^c\",\"C\",temp); temp=gsub(\"^g\",\"G\",temp); \r\n  temp=gsub(\"^k\",\"K\",temp); temp=gsub(\"^m\",\"M\",temp); temp=gsub(\"^n\",\"N\",temp); temp=gsub(\"^p\",\"P\",temp);\r\n  temp=gsub(\"^r\",\"R\",temp); temp=gsub(\"^s\",\"S\",temp); temp=gsub(\"^t\",\"T\",temp); temp=gsub(\"^u\",\"U\",temp);\r\n  temp=gsub(\"^v\",\"V\",temp); temp=gsub(\"^y\",\"Y\",temp); temp=gsub(\"_a\",\"A\",temp); temp=gsub(\"_b\",\"B\",temp);\r\n  temp=gsub(\"_c\",\"C\",temp); temp=gsub(\"_d\",\"D\",temp); temp=gsub(\"_f\",\"F\",temp); temp=gsub(\"_g\",\"G\",temp);\r\n  temp=gsub(\"_k\",\"K\",temp); temp=gsub(\"_m\",\"M\",temp); temp=gsub(\"_n\",\"N\",temp); temp=gsub(\"_p\",\"P\",temp);\r\n  temp=gsub(\"_r\",\"R\",temp); temp=gsub(\"_s\",\"S\",temp); temp=gsub(\"_t\",\"T\",temp); temp=gsub(\"_u\",\"U\",temp);\r\n  temp=gsub(\"_v\",\"V\",temp); temp=gsub(\"_x\",\"X\",temp); temp=gsub(\"_y\",\"Y\",temp); temp=gsub(\"_w\",\"W\",temp);\r\n  temp=gsub(\"_z\",\"Z\",temp); temp=gsub(\"_1\",\"1\",temp); temp=gsub(\"_2\",\"2\",temp);\r\n  TidyDataTest = pml.testing\r\n  colnames(TidyDataTest) = tempt\r\n  NewTidyDataTest = TidyDataTest[, colSums(is.na(TidyDataTest))/nrow(TidyDataTest) < 0.95]\r\n  dim(NewTidyDataTest)\r\n```\r\n\r\n```R\r\n  [1] 20 60\r\n```\r\n```R\r\n  library(dplyr)\r\n  CaseDataTest = select(NewTidyDataTest,-c(X:NumWindow))\r\n  dim(CaseDataTest)\r\n  [1] 20 53\r\n```\r\n\r\n```R\r\n  predict(modelFit2, newdata = CaseDataTest)\r\n```\r\n```R\r\n [1] B A B A A E D B A A B C B A E E A B B B\r\nLevels: A B C D E\r\n```\r\nI have also evaluated the other 4 models:  modelFit2a, modelFit2b, modelFit2c and modelFit2f. These models presented slightly different inaccuracies and as expected, all gave the same results.\r\n\r\n###Conclusions:\r\nIn this report, how to download the data is shown and also how to review it by analysing the data structure. The data was also tidied up and reorganised for better variable description (CamelCase). Correlations between the variables to reduce the model complexity were performed. The main assumptions made for the proposed model is to be person-independent and time-independent. The initial 160 variables were reduced to 46. Final models show that only 12 predictors could be used. Also, how to build a Machine Learning Model by using *caret* package was shown, along with how the parameters in the `train()` function can affect the model accuracy and fitting time. For instance, k-fold cross validation gave similar results in terms of accuracy but the modelling of the training set was much faster than by using boostrap resampling. In all the cases, the number of predictors used in the final model was the same, that is, 23. The best sample error obtained was 0.68% with the original model. Finally, by using the Random Forest method, all the models in this work predict the same result in the 20 test cases, that is:\r\n```R\r\n  table(predict(modelFit2f, newdata = CaseDataTest))\r\n```\r\n```R\r\nA B C D E \r\n7 8 1 1 3 \r\n```\r\nOnly 7 out of 20 shows the right way to perform the dumbbell lifting exercise.\r\n\r\n\r\n\r\n***************************************************\r\n####Reference:\r\n\r\nhttps://perceptual.mpi-inf.mpg.de/files/2013/03/velloso13_ah.pdf\r\n\r\nhttps://www.sparkfun.com/products/10736\r\n\r\nhttp://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/BuildingPredictiveModelsR_caret.pdf\r\n\r\nhttp://topepo.github.io/caret/training.html\r\n\r\nhttp://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram\r\n\r\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#hr\r\n\r\nhttp://hplgit.github.io/teamods/bitgit/html-github/._main_bitgit002.html\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}